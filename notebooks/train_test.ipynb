{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 00:04:01.053710: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-18 00:04:01.057618: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-18 00:04:01.067397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-18 00:04:01.083597: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-18 00:04:01.088339: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-18 00:04:01.101944: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-18 00:04:01.992072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from arch import arch_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import yfinance as yf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.parameters import STR_CONN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql(\"SELECT symbol, date, close FROM stock_prices WHERE symbol='META'\", con=STR_CONN)\n",
    "data = data.set_index('date').loc[:, ['close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=0.2):\n",
    "    return train_test_split(data, test_size=test_size, shuffle=False)\n",
    "\n",
    "def train_test_arima(train, test, order=(1,1,1)):\n",
    "    model = ARIMA(train, order=order)\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=len(test))\n",
    "    mse = mean_squared_error(test, forecast)\n",
    "    return model_fit, forecast, mse\n",
    "\n",
    "def train_test_garch(train, test, p=1, q=1):\n",
    "    model = arch_model(train, vol='GARCH', p=p, q=q)\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(horizon=len(test))\n",
    "    mse = mean_squared_error(test, forecast.mean.iloc[-1])\n",
    "    return model_fit, forecast.mean.iloc[-1], mse\n",
    "\n",
    "# Modelo LSTM\n",
    "def train_test_lstm(train, test, look_back=1):\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train.values.reshape(-1, 1))\n",
    "    \n",
    "    def create_dataset(dataset, look_back=1):\n",
    "        X, y = [], []\n",
    "        for i in range(len(dataset) - look_back):\n",
    "            X.append(dataset[i:(i + look_back), 0])\n",
    "            y.append(dataset[i + look_back, 0])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    X_train, y_train = create_dataset(train_scaled, look_back)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(look_back, 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
    "    \n",
    "    test_scaled = scaler.transform(test.values.reshape(-1, 1))\n",
    "    X_test, y_test = create_dataset(test_scaled, look_back)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    mse = mean_squared_error(test[look_back:], predictions)\n",
    "    \n",
    "    return model, predictions, mse\n",
    "\n",
    "def train_test_minirocket_lgbm(train, test, num_kernels=1000):\n",
    "    train_2d = train.values.reshape((1, 1, -1))\n",
    "    test_2d = test.values.reshape((1, 1, -1))\n",
    "    \n",
    "    rocket = MiniRocketMultivariate(num_kernels=num_kernels)\n",
    "    rocket.fit(train_2d)\n",
    "    train_transform = rocket.transform(train_2d)\n",
    "    test_transform = rocket.transform(test_2d)\n",
    "    \n",
    "    model = LGBMRegressor()\n",
    "    model.fit(train_transform, train.values)\n",
    "    \n",
    "    predictions = model.predict(test_transform)\n",
    "    mse = mean_squared_error(test, predictions)\n",
    "    \n",
    "    return model, predictions, mse\n",
    "\n",
    "def train_test_lgbm(train, test, look_back=10):\n",
    "    def create_features(data, look_back):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - look_back):\n",
    "            X.append(data[i:i+look_back])\n",
    "            y.append(data[i+look_back])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    X_train, y_train = create_features(train.values, look_back)\n",
    "    X_test, y_test = create_features(test.values, look_back)\n",
    "    \n",
    "    model = LGBMRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    \n",
    "    return model, predictions, mse\n",
    "\n",
    "def train_test_xgboost(train, test, look_back=10):\n",
    "    def create_features(data, look_back):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - look_back):\n",
    "            X.append(data[i:i+look_back])\n",
    "            y.append(data[i+look_back])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    X_train, y_train = create_features(train.values, look_back)\n",
    "    X_test, y_test = create_features(test.values, look_back)\n",
    "    \n",
    "    model = XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    \n",
    "    return model, predictions, mse\n",
    "\n",
    "def run_all_models(data):\n",
    "    train, test = split_data(data)\n",
    "    results = {}\n",
    "    \n",
    "    # ARIMA\n",
    "    arima_model, arima_forecast, arima_mse = train_test_arima(train, test)\n",
    "    results['ARIMA'] = {'model': arima_model, 'forecast': arima_forecast, 'mse': arima_mse}\n",
    "    \n",
    "    # GARCH\n",
    "    garch_model, garch_forecast, garch_mse = train_test_garch(train, test)\n",
    "    results['GARCH'] = {'model': garch_model, 'forecast': garch_forecast, 'mse': garch_mse}\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_model, lstm_predictions, lstm_mse = train_test_lstm(train, test)\n",
    "    results['LSTM'] = {'model': lstm_model, 'predictions': lstm_predictions, 'mse': lstm_mse}\n",
    "    \n",
    "    # # MiniRocket + LGBM\n",
    "    # minirocket_lgbm_model, minirocket_lgbm_predictions, minirocket_lgbm_mse = train_test_minirocket_lgbm(train, test)\n",
    "    # results['MiniRocket+LGBM'] = {'model': minirocket_lgbm_model, 'predictions': minirocket_lgbm_predictions, 'mse': minirocket_lgbm_mse}\n",
    "    \n",
    "    # LGBM\n",
    "    # lgbm_model, lgbm_predictions, lgbm_mse = train_test_lgbm(train, test)\n",
    "    # results['LGBM'] = {'model': lgbm_model, 'predictions': lgbm_predictions, 'mse': lgbm_mse}\n",
    "    \n",
    "    # XGBoost\n",
    "    # xgboost_model, xgboost_predictions, xgboost_mse = train_test_xgboost(train, test)\n",
    "    # results['XGBoost'] = {'model': xgboost_model, 'predictions': xgboost_predictions, 'mse': xgboost_mse}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql(\"SELECT symbol, date, close FROM stock_prices WHERE symbol='META'\", con=STR_CONN)\n",
    "data = data.set_index('date').loc[:, ['close']]\n",
    "\n",
    "train, test = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:      1,   Func. Count:      6,   Neg. LLF: 11498.697263621332\n",
      "Iteration:      2,   Func. Count:     12,   Neg. LLF: 2271.4626766960737\n",
      "Iteration:      3,   Func. Count:     17,   Neg. LLF: 2245.7253430494175\n",
      "Iteration:      4,   Func. Count:     22,   Neg. LLF: 2235.528181109994\n",
      "Iteration:      5,   Func. Count:     27,   Neg. LLF: 2232.393094977644\n",
      "Iteration:      6,   Func. Count:     32,   Neg. LLF: 2232.1725597402587\n",
      "Iteration:      7,   Func. Count:     37,   Neg. LLF: 2232.0589830231606\n",
      "Iteration:      8,   Func. Count:     42,   Neg. LLF: 2232.0431925461\n",
      "Iteration:      9,   Func. Count:     47,   Neg. LLF: 2232.0023774260817\n",
      "Iteration:     10,   Func. Count:     52,   Neg. LLF: 2231.9153681319726\n",
      "Iteration:     11,   Func. Count:     57,   Neg. LLF: 2231.6621545035323\n",
      "Iteration:     12,   Func. Count:     62,   Neg. LLF: 2231.004214335363\n",
      "Iteration:     13,   Func. Count:     67,   Neg. LLF: 2229.1733521540637\n",
      "Iteration:     14,   Func. Count:     72,   Neg. LLF: 2223.9691550409552\n",
      "Iteration:     15,   Func. Count:     77,   Neg. LLF: 2395.3503377956363\n",
      "Iteration:     16,   Func. Count:     83,   Neg. LLF: 2860.3052152317405\n",
      "Iteration:     17,   Func. Count:     89,   Neg. LLF: 5673.634551678318\n",
      "Iteration:     18,   Func. Count:     95,   Neg. LLF: 13603.471063354398\n",
      "Iteration:     19,   Func. Count:    101,   Neg. LLF: 2584.103433372273\n",
      "Iteration:     20,   Func. Count:    107,   Neg. LLF: 4474.618991733564\n",
      "Iteration:     21,   Func. Count:    113,   Neg. LLF: 3491.6493836192803\n",
      "Iteration:     22,   Func. Count:    119,   Neg. LLF: 2352.9921196445694\n",
      "Iteration:     23,   Func. Count:    125,   Neg. LLF: 2267.041510493341\n",
      "Iteration:     24,   Func. Count:    131,   Neg. LLF: 2250.654147133161\n",
      "Iteration:     25,   Func. Count:    137,   Neg. LLF: 2246.61644082724\n",
      "Iteration:     26,   Func. Count:    143,   Neg. LLF: 2243.492224220053\n",
      "Iteration:     27,   Func. Count:    149,   Neg. LLF: 2240.652620840464\n",
      "Iteration:     28,   Func. Count:    155,   Neg. LLF: 2201.0295224952383\n",
      "Iteration:     29,   Func. Count:    161,   Neg. LLF: 2263.6164651118734\n",
      "Iteration:     30,   Func. Count:    167,   Neg. LLF: 2201.1208870971154\n",
      "Iteration:     31,   Func. Count:    173,   Neg. LLF: 2199.3883335136425\n",
      "Iteration:     32,   Func. Count:    178,   Neg. LLF: 2199.3571359492694\n",
      "Iteration:     33,   Func. Count:    183,   Neg. LLF: 2199.34110975977\n",
      "Iteration:     34,   Func. Count:    188,   Neg. LLF: 2199.33744600276\n",
      "Iteration:     35,   Func. Count:    193,   Neg. LLF: 2199.3373797254244\n",
      "Iteration:     36,   Func. Count:    198,   Neg. LLF: 2199.3373786262973\n",
      "Iteration:     37,   Func. Count:    202,   Neg. LLF: 2199.337378626262\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 2199.3373786262973\n",
      "            Iterations: 37\n",
      "            Function evaluations: 202\n",
      "            Gradient evaluations: 37\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "ARIMA MSE: 786.3565559698337\n",
      "GARCH MSE: 37319.279867529054\n",
      "LSTM MSE: 119.88948651680722\n",
      "El mejor modelo es: LSTM\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Predicción para los próximos 30 días usando LSTM:\n",
      "[[6.8888125]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = run_all_models(data)\n",
    "\n",
    "# Imprimir resultados\n",
    "for model_name, model_results in results.items():\n",
    "    print(f\"{model_name} MSE: {model_results['mse']}\")\n",
    "\n",
    "# Para hacer predicciones futuras, use el modelo con el MSE más bajo\n",
    "best_model = min(results, key=lambda x: results[x]['mse'])\n",
    "print(f\"El mejor modelo es: {best_model}\")\n",
    "\n",
    "# Ejemplo de predicción futura (asumiendo que queremos predecir los próximos 30 días)\n",
    "# Nota: Este es un ejemplo simplificado. En la práctica, necesitarías ajustar esto según el modelo específico\n",
    "future_days = 30\n",
    "best_model_data = results[best_model]\n",
    "if best_model in ['ARIMA', 'GARCH']:\n",
    "    future_prediction = best_model_data['model'].forecast(steps=future_days)\n",
    "else:\n",
    "    # Para los otros modelos, necesitarías preparar los datos de entrada adecuadamente\n",
    "    # Este es solo un placeholder y necesitaría ser ajustado según el modelo específico\n",
    "    last_known_data = results[best_model]['predictions'][-10:]  # Últimos 10 puntos de datos\n",
    "    future_prediction = best_model_data['model'].predict(np.array([last_known_data]))\n",
    "\n",
    "print(f\"Predicción para los próximos {future_days} días usando {best_model}:\")\n",
    "print(future_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance_dash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
